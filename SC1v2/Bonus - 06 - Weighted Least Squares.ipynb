{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"http://sct.inf.utfsm.cl/wp-content/uploads/2020/04/logo_di.png\" style=\"width:60%\">\n",
    "    <h1> INF285 - Computación Científica </h1>\n",
    "    <h2> Weigthed Linear Least Squares Problems</h2>\n",
    "    <h2> <a href=\"#acknowledgements\"> [S]cientific [C]omputing [T]eam </a> </h2>\n",
    "    <h2> Version: 1.04</h2>\n",
    "</center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='toc' />\n",
    "\n",
    "## Table of Contents\n",
    "* [Introduction](#intro)\n",
    "* [Weighted least-square](#wLeastSquare)\n",
    "    * [Example in explanation](#exampleExplanation)\n",
    "    * [Extension of \"Initial Example\" in jupyter notebook \"07_08_Least_Squares\"](#extensionInitialExample)\n",
    "* [Acknowledgements](#acknowledgements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg as spla\n",
    "%matplotlib inline\n",
    "# https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets\n",
    "from sklearn import datasets\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.size'] = 14\n",
    "mpl.rcParams['axes.labelsize'] = 20\n",
    "mpl.rcParams['xtick.labelsize'] = 14\n",
    "mpl.rcParams['ytick.labelsize'] = 14\n",
    "M=8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='intro' />\n",
    "\n",
    "## Introduction\n",
    "[Back to TOC](#toc)\n",
    "\n",
    "This jupyter notebook presents the notion of weigthed linear least square problems.\n",
    "It first present the theoretical background, then a few small examples and finally makes a connection with the \"Initial Example\" from the section \"Overdetermined Linear Systems of Equations\" in the notebook \"07_08_Least_Squares\".\n",
    "We strongly suggest the reader to review that example first."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='wLeastSquare' />\n",
    "\n",
    "## Weighted Linear Least-Square Problems\n",
    "[Back to TOC](#toc)\n",
    "\n",
    "In this example we will consider we give a different weight to each term in the linear least square problem.\n",
    "Mathematical, this can be seen as multiplying the least-square problem by a diagonal matrix, say $W=\\text{diag}(w_1,w_2,\\dots,w_m)$, with $w_i>0$.\n",
    "For instance, consider the following least square example:\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\widehat{\\mathbf{r}} = W\\,\\mathbf{r} =\n",
    "    \\underbrace{W}_{\\texttt{Weight matrix}}\\,\\left(\\underbrace{\\begin{bmatrix}\n",
    "        y_1 \\\\\n",
    "        y_2 \\\\\n",
    "        y_3 \\\\\n",
    "        \\vdots\\\\\n",
    "        y_m \n",
    "    \\end{bmatrix}}_{\\displaystyle{\\mathbf{b}}}\n",
    "    -\n",
    "    \\underbrace{\\begin{bmatrix}\n",
    "        1 & x_1 \\\\\n",
    "        1 & x_2 \\\\\n",
    "        1 & x_3 \\\\\n",
    "        \\vdots & \\vdots \\\\\n",
    "        1 & x_m\n",
    "    \\end{bmatrix}}_{\\displaystyle{A}}\n",
    "    \\underbrace{\\begin{bmatrix}\n",
    "        a\\\\\n",
    "        b\n",
    "    \\end{bmatrix}}_{\\mathbf{x}}\\right).\n",
    "\\end{equation}\n",
    "$$\n",
    "In this case, if $W$ is the identity matrix, all the equations have the same weight.\n",
    "Now, giving a different weight to each equation, we obtain the following:\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\underbrace{W\\,A}_{\\displaystyle{B}}\\,\\mathbf{x}=\\underbrace{W\\,\\mathbf{b}}_{\\displaystyle{\\mathbf{c}}},\n",
    "\\end{equation}\n",
    "$$\n",
    "where, as mentioned before, we could define $W$ as follows,\n",
    "$$\n",
    "\\begin{equation}\n",
    "    W=\\text{diag}(w_1,w_2,\\dots,w_m).\n",
    "\\end{equation}\n",
    "$$\n",
    "So, when computing the quadratic error we obtain,\n",
    "$$\\begin{equation}\n",
    "    E = \\left\\|\\widehat{\\mathbf{r}} \\right\\|_2^2 =  \\left\\|W\\,\\mathbf{r} \\right\\|_2^2 = \\left\\|W\\,\\left(\\mathbf{b}- A\\,\\mathbf{x}\\right)\\right\\|_2^2=\\sum_{i=1}^m w_i^2\\,(y_i-a-b\\,x_i)^2.\n",
    "\\end{equation}\n",
    "$$\n",
    "This indicates that when we use a higher value of $w_i$ for some $i$'s, these equations will have a higher impact in the minimization.\n",
    "From the normal equations point of view, this slightly modified problem still needs to satisfy the normal equations, which in this case become,\n",
    "$$\n",
    "\\begin{equation}\n",
    "    B^*\\,B\\,\\overline{\\mathbf{x}}_w=B^*\\,\\mathbf{c}.\n",
    "\\end{equation}\n",
    "$$\n",
    "Notice that we used the sub-index $w$ for $\\overline{\\mathbf{x}}_w$ to differentiate it from the unweighted least square solution.\n",
    "Thus, caming back to the original equations, we obtain,\n",
    "$$\n",
    "\\begin{align*}\n",
    "    B^*\\,B\\,\\overline{\\mathbf{x}}_w &=B^*\\,\\mathbf{c},\\\\\n",
    "    (W\\,A)^*\\,(W\\,A)\\,\\overline{\\mathbf{x}}_w &=(W\\,A)^*\\,W\\,\\mathbf{b},\\\\\n",
    "    A^*\\,W^*\\,W\\,A\\,\\overline{\\mathbf{x}}_w &=A^*\\,W^*\\,W\\,\\mathbf{b},\\\\\n",
    "    A^*\\,W^2\\,A\\,\\overline{\\mathbf{x}}_w &=A^*\\,W^2\\,\\mathbf{b},\\\\\n",
    "    \\overline{\\mathbf{x}}_w &=(A^*\\,W^2\\,A)^{-1}\\,A^*\\,W^2\\,\\mathbf{b}.\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "So, if $W$ is the identity matrix or a non-zero multiple of it, i.e. $W=\\alpha\\,I$ for $\\alpha\\neq0$, the weigthed least square problem reduces to,\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\overline{\\mathbf{x}} &=(A^*\\,\\alpha^2\\,I^2\\,A)^{-1}\\,A^*\\,\\alpha^2\\,I^2\\,\\mathbf{b},\\\\\n",
    "    \\overline{\\mathbf{x}} &=(A^*\\,A)^{-1}\\,A^*\\,\\mathbf{b},\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "which means that it obtains the tradicional linear least-square apporximation.\n",
    "**However, if the weights are not equal, we get a different approximation**.\n",
    "\n",
    "It is important to point out that, from the least equare point of view, any algebraic modification we do to any equation, it will change its respective weight respect to the other equations.\n",
    "For instance, if we have the following problem,\n",
    "$$\n",
    "\\begin{equation}\n",
    "   \\widetilde{\\mathbf{r}} = W\\,\\mathbf{r}=\n",
    "    \\begin{bmatrix}\n",
    "        4\\,y_1 \\\\\n",
    "        y_2 \\\\\n",
    "        y_3 \n",
    "    \\end{bmatrix}\n",
    "    -\n",
    "    \\begin{bmatrix}\n",
    "        4 & 4\\,x_1 \\\\\n",
    "        1 & x_2 \\\\\n",
    "        1 & x_3\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        a\\\\\n",
    "        b\n",
    "    \\end{bmatrix}.\n",
    "\\end{equation}\n",
    "$$\n",
    "It will have a different least square solution since we have a weight of $4$ for the first equation.\n",
    "This is because we are minimizing the residual $\\left\\|\\widetilde{\\mathbf{r}}\\right\\|_2^2$.\n",
    "\n",
    "On the other hand, if we had cancelled out the coefficient $4$, we would be solving,\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\widehat{\\mathbf{r}}\n",
    "    =\n",
    "    \\begin{bmatrix}\n",
    "        y_1 \\\\\n",
    "        y_2 \\\\\n",
    "        y_3 \n",
    "    \\end{bmatrix}\n",
    "    -\n",
    "    \\begin{bmatrix}\n",
    "        1 & x_1 \\\\\n",
    "        1 & x_2 \\\\\n",
    "        1 & x_3\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        a\\\\\n",
    "        b\n",
    "    \\end{bmatrix},\n",
    "\\end{equation}\n",
    "$$\n",
    "which minimizes the norm of a different residual vector, which is $\\left\\|\\mathbf{r}\\right\\|_2^2$.\n",
    "Therefore, this is why they generate different linear least-square sollutions.\n",
    "\n",
    "**It is very important to hightlight that this weight effect is important because we are dealing with overdetermined linear system of equation.**\n",
    "In the case we deal with an **square and non-singular linear system of equations this does not change the solution** in exact arithmetic, for instance, the analysis changes to,\n",
    "$$\n",
    "\\begin{align*}\n",
    "    B^*\\,B\\,\\mathbf{x} &= B^*\\,\\mathbf{c},\\\\\n",
    "    (W\\,A)^*\\,(W\\,A)\\,\\mathbf{x} &= (W\\,A)^*\\,W\\,\\mathbf{b},\\\\\n",
    "    A^*\\,W^*\\,W\\,A\\,\\mathbf{x} &=A^*\\,W^*\\,W\\,\\mathbf{b},\\\\\n",
    "    A^*\\,W^2\\,A\\,\\mathbf{x} &=A^*\\,W^2\\,\\mathbf{b},\\\\\n",
    "    \\mathbf{x} &=(A^*\\,W^2\\,A)^{-1}\\,A^*\\,W^2\\,\\mathbf{b},\\\\\n",
    "    \\mathbf{x} &=A^{-1}\\,W^{-2}\\,A^{-*}\\,A^*\\,W^2\\,\\mathbf{b},\\\\\n",
    "    \\mathbf{x} &=A^{-1}\\,W^{-2}\\,\\,W^2\\,\\mathbf{b},\\\\\n",
    "    \\mathbf{x} &=A^{-1}\\,\\mathbf{b}.\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "The key difference is on the sixth line.\n",
    "In this case there exists the inverse matrix of each matrix in the parenthesis, which was not true previously."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='exampleExplanation' />\n",
    "\n",
    "### Example in explanation\n",
    "[Back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1:  [[4. 4.]\n",
      " [1. 2.]\n",
      " [1. 3.]]\n",
      "b1:  [20 -1  3]\n",
      "x1_bar:  [ 6.80246914 -1.92592593]\n",
      "A2:  [[1. 1.]\n",
      " [1. 2.]\n",
      " [1. 3.]]\n",
      "b2:  [ 5 -1  3]\n",
      "x2_bar:  [ 4.33333333 -1.        ]\n"
     ]
    }
   ],
   "source": [
    "x1,x2,x3 = 1,2,3\n",
    "y1,y2,y3 = 5,-1,3\n",
    "A1 = np.ones((3,2))\n",
    "A1[:,1]=[x1,x2,x3]\n",
    "b1=np.array([y1,y2,y3])\n",
    "A1[0,:]*=4\n",
    "b1[0]*=4\n",
    "x1_bar=np.linalg.solve(A1.T @ A1,A1.T @ b1)\n",
    "print('A1: ', A1)\n",
    "print('b1: ', b1)\n",
    "print('x1_bar: ', x1_bar)\n",
    "\n",
    "A2 = np.ones((3,2))\n",
    "A2[:,1]=[x1,x2,x3]\n",
    "b2=np.array([y1,y2,y3])\n",
    "x2_bar=np.linalg.solve(A2.T @ A2,A2.T @ b2)\n",
    "print('A2: ', A2)\n",
    "print('b2: ', b2)\n",
    "print('x2_bar: ', x2_bar)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clearly observe that the least square solutions are different!\n",
    "This is consistent with the previous explanation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='extensionInitialExample' />\n",
    "\n",
    "### Extension of \"Initial Example\" in jupyter notebook \"07_08_Least_Squares\"\n",
    "[Back to TOC](#toc)\n",
    "\n",
    "\n",
    "In this example we will approximate $m$ points considering a linear relationship.\n",
    "This means that we will have the data points $(x_i,y_i)$ for $i\\in\\{1,2,\\dots,m\\}$ and consider the relationshop $y=a_0+a_1\\,b$.\n",
    "The error that will be added follows a normal distribution, but we will consider we use the following weight matrix,\n",
    "$$\n",
    "\\begin{equation}\n",
    "    W=\\text{diag}(w,w,1,\\dots,1).\n",
    "\\end{equation}\n",
    "$$\n",
    "This means we will more weight to the first twio equations, this is arbitrarily, and it is only use to show what effects brings to the least square problem.\n",
    "\n",
    "The example will only show the approximation output since we already saw all the other components in the original example.\n",
    "\n",
    "### Question to think before to modify the weight $w$: What would you expect to happen with the approximation if you use a large value for $w$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25c96248f11849d09947b6076a4dcd9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='w', max=101.0, min=0.01, step=0.01), Output()), _dom…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.showWeightedOutput(w=1)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def showWeightedOutput(w=1):\n",
    "    # Number of points to be used\n",
    "    m = 10\n",
    "    # Relationship considered\n",
    "    fv = np.vectorize(lambda x, a0, a1: a0+a1*x)\n",
    "    # Coefficients considered\n",
    "    a0, a1 = 1, 4\n",
    "\n",
    "    np.random.seed(0)\n",
    "    # Standard deviation for the error\n",
    "    sigma = 5e-1\n",
    "    # Error to be added\n",
    "    e = np.random.normal(0,sigma,m)\n",
    "\n",
    "    # Generating data points\n",
    "    x = np.linspace(0,1,m)\n",
    "    y = fv(x,a0,a1)+e\n",
    "\n",
    "    # Build the data matrix\n",
    "    A = np.ones((m,2))\n",
    "    A[:,1] = x\n",
    "    # Setting up the right hand side\n",
    "    b = np.copy(y)\n",
    "    A[:2,:]*=w\n",
    "    b[:2]*=w\n",
    "\n",
    "    # Building and solving the normal equations\n",
    "    # A^T A x_bar = A^T b\n",
    "    x_bar = np.linalg.solve(A.T @ A, A.T @ b)\n",
    "    # Showing the comparison between the \"original function\" and the \"least-squared reconstructed approximation\".\n",
    "    # We added in red a \"sample\" of possible functions.\n",
    "    # Notice that the colors used follow the description included in the classnotes.\n",
    "    # This means to consider the following analogy:\n",
    "    # blue: data points, this correspond to the right-hand-side vector \"b\".\n",
    "    # red: this correspond to the sub-space generated by Ax, i.e. the span of the columns of A.\n",
    "    # violet: This correspond to the least-square solution found.\n",
    "    ####\n",
    "    # plt.figure(figsize=(10,10))\n",
    "    # for i in range(100):\n",
    "    #     plt.plot(x,fv(x,x_bar[0]+np.random.normal(0,1),x_bar[1]+np.random.normal(0,1)),'r-',linewidth=1,alpha=0.2)\n",
    "    # plt.plot(x,fv(x,a0,a1),'k-',linewidth=8,alpha=0.8)\n",
    "    # plt.plot(x,fv(x,x_bar[0],x_bar[1]),'--',color='darkviolet',linewidth=4)\n",
    "    # plt.plot(x,fv(x,x_bar[0],x_bar[1]),'r.',markersize=20)\n",
    "    # plt.plot(x,y,'b.',markersize=10)\n",
    "    # plt.grid(True)\n",
    "    # plt.xlabel(r'$x$')\n",
    "    # plt.ylabel(r'$y$')\n",
    "    #\n",
    "    plt.plot(x,y,'b.',markersize=20, label='Data points')\n",
    "    plt.plot(x,fv(x,x_bar[0],x_bar[1]),'r.', markersize=30, label='Approximated points')\n",
    "    plt.plot(x,fv(x,a0,a1),'k-',linewidth=8,alpha=0.8, label='Original function')\n",
    "    plt.plot(x,fv(x,x_bar[0],x_bar[1]),'--',color='darkviolet',linewidth=4, label='Least Square approximation')\n",
    "    plt.plot(x,fv(x,x_bar[0]+np.random.normal(0,1),x_bar[1]+np.random.normal(0,1)),'r-',linewidth=1,alpha=0.2, label='Non-optimal solutions')\n",
    "    plt.grid(True)\n",
    "    plt.xlabel(r'$x$')\n",
    "    plt.ylabel(r'$y$')\n",
    "    plt.legend(loc='lower left', ncol=1, fancybox=True, shadow=True, numpoints=1, bbox_to_anchor=(1,0))\n",
    "    #\n",
    "    plt.show()\n",
    "interact(showWeightedOutput,w=(0.01,101,0.01))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='acknowledgements' />\n",
    "\n",
    "# Acknowledgements\n",
    "[Back to TOC](#toc)\n",
    "* _Material created by professor Claudio Torres_ (`ctorres@inf.utfsm.cl`) DI UTFSM. June 2021.- v1.0.\n",
    "* _Update May 2022 - v1.01 - C.Torres_ : Adding \\$\\$ in Markdown for LaTeX.\n",
    "* _Update May 2024 - v1.02 - C.Torres_ : Updating the use of strings for Python 3.12.*.\n",
    "* _Update Nov 2024 - v1.03 - C.Torres_ : Including now the use of the residual vector.\n",
    "* _Update Nov 2024 - v1.04 - C.Torres_ : Updating the explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "widgets": {
   "state": {
    "7fd91d6f0d2545e7af10aae93cfe07e9": {
     "views": [
      {
       "cell_index": 15
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
